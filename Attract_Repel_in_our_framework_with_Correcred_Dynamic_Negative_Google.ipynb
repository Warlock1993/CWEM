{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\warlo\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.util import ngrams\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['during', 'trip', 'operation', 'damage', 'wa', 'observe', 'on']\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "def build_dataset(words, n_words): # n_words is the size of the vocabulary that we want taken as 10000 here\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]] # initialise a list that counts the occurance of each word\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1)) # fill the list with each word count as a tuple (word, word_count)\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary) # create a dictionary where keys are the words and their values are their rank of counts eg. as:1 if as is ranked 1 in the count list\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    # the created index for each word (based on its count rank) is now stored in the list named \"data\"\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word] # index of the word being initialised as per the count of the word\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index) # contains index of each word based on its count rank \n",
    "                        # BUT NOTE THE INDEX ARE NOT ORDERED AS PER COUNTS AND PRESERVE THEIR ORIGNINAL ORDER IN THE SENTENCES\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "def collect_data(vocabulary_size=13648):\n",
    "    filename = 'filename.zip'\n",
    "    vocabulary = read_data(filename) #returns a list of all words in the text file\n",
    "    print(vocabulary[:7])\n",
    "    data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
    "                                                                vocabulary_size) # Provide the list of words and vocabulary size to create the dataset\n",
    "    del vocabulary  # Hint to reduce memory.\n",
    "    # the data contains list of indexes of each word as per their occurence sequence in the text file\n",
    "    # Try printing print(reverse_dictionary[5234],reverse_dictionary[3081],reverse_dictionary[12],reverse_dictionary[6],reverse_dictionary[195]) \n",
    "    \n",
    "    # the count is a list of tuples (word, word_count)\n",
    "    # dictionary contains keys as words and their index as values\n",
    "    # dictionary contains keys as index and their corresponding words as values\n",
    "    \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data_index = 0 # initialize data index and maake it global to use in the function bellow\n",
    "# generate batch data\n",
    "def generate_batch(data, batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0 # proceed only if the condition is true \" BATCH SIZE IS A MULTIPLE OF num_skips\"\n",
    "    assert num_skips <= 2 * skip_window # number of words to be randomly picked from surrounding the context word should be less than equal to skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32) # initialize a nd.array for the batch of input with size (batch_size,) eg. (128,)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32) # initialize a nd.array for the context with size (batch_size,1) \n",
    "                    # eg. (128, 1) to hold index of context words for each word (index) in batch array([[  18],[  18],[   1],[   1],..])\n",
    "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
    "    buffer = collections.deque(maxlen=span) # A double ended queu where elements can be added or subtracted from both ends\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index]) # data_index is initiallized by 0 outsuide the function\n",
    "        data_index = (data_index + 1) % len(data) # after this step in the buffer 5 words are stored based on their index in the supplied data list\n",
    "                            # The data list contains index of each word in the order of sequence they occur in the text and thus it preserve the order of the context \n",
    "    for i in range(batch_size // num_skips): # batch_size divided by num_skips =128/2 = 64\n",
    "        target = skip_window  # input word at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1) #generates a random integer between 0-4 so that the index of the words surrounding the input word can be obtained\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word ## at the between of the buffer\n",
    "            context[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
    "        buffer.append(data[data_index]) # find the comment bellow\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, context\n",
    "\n",
    "'''\n",
    "This buffer will hold a maximum of span elements and will be a kind of moving window of words that samples are drawn from. \n",
    "Whenever a new word index is added to the buffer, the left most element will drop out of the buffer to allow room for the new \n",
    "word index being added.  The position of the buffer in the input text stream is stored in a global variable data_index which is\n",
    "incremented each time a new word is added to the buffer.  If it gets to the end of the text stream, the “% len(data)” component \n",
    "of the index update will basically reset the count back to zero.\n",
    "'''\n",
    "\n",
    "vocabulary_size=13648\n",
    "data, count, dictionary, reverse_dictionary = collect_data(vocabulary_size=13648)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 100  # Dimension of the embedding vector.\n",
    "skip_window = 5       # How many words to consider left and right.\n",
    "num_skips = 0         # How many times to reuse an input to generate a label. \n",
    "\n",
    "## By reading the blog the understanding of num_skips is that this parameter suggests the\n",
    "## number of words to be randomly picked from the words in the context neighborhood of 'sat'\n",
    "\n",
    "'''\n",
    "For instance, in the 5-gram “the cat sat on the”, the input word will be center word i.e. “sat” and \n",
    "the context words that will be predicted will be drawn randomly from the remaining words of the gram: \n",
    "[‘the’, ‘cat’, ‘on1’, ‘the’].  In this function, the number of words drawn randomly from the surrounding \n",
    "context is defined by the argument num_skips.  The size of the window of context words to draw from around'\n",
    "the input word is defined in the argument skip_window – in the example above (“the cat sat on the”), we have \n",
    "a skip window width of 2 around the input word “sat”.\n",
    "'''\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 15    # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting word embeddings from google\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "#again, download and load the model\n",
    "model_gigaword = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"glove-wiki-gigaword-100.vocab\", \"w\", encoding='utf-8') as f:\n",
    "        for _, w in sorted((voc.index, word) for word, voc in model_gigaword.vocab.items()):\n",
    "            print(w, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"glove-wiki-gigaword-100.vocab\", encoding='utf-8') as f:\n",
    "    vocab_list = map(str.strip, f.readlines())\n",
    "    \n",
    "vocab_dict = {w: k for k, w in enumerate(vocab_list)}\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\warlo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "embedding_google = model_gigaword.syn0\n",
    "np.save('Embeddings_generated_Google_25_MAY', embedding_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the dictionary\n",
    "import json\n",
    "json = json.dumps(vocab_dict)\n",
    "f = open(\"word_index_dictionary_for_Google_embeddings_25_MAY.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "wnl = WordNetLemmatizer()\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n'    \n",
    "def lemmatize_sent(text): \n",
    "    # Text input is string, returns lowercased strings. \n",
    "    words_lement = [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) for word, tag in pos_tag(word_tokenize(text))]\n",
    "    combined_string = ' '.join(map(str, words_lement))\n",
    "    return  combined_string # ' '.join([for words in words_lement]) #' '.join([wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) for word, tag in pos_tag(word_tokenize(text))])\n",
    "\n",
    "lementize_words =  lambda x: lemmatize_sent(str(x))\n",
    "# fail_mech['lementized_form_words'] = fail_mech['Tokens'].apply(lementize_words)\n",
    "# fail_mech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_google= np.load('Embeddings_generated_Google_25_MAY.npy')\n",
    "norm_google=np.sqrt(np.sum(np.square(embeddings_google), axis=1))\n",
    "embeddings_google_scaled=embeddings_google/norm_google[:,None]\n",
    "import json\n",
    "with open('word_index_dictionary_for_Google_embeddings_25_MAY.json', 'r') as fp:\n",
    "    vocab_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('equipment_dictionary.json', 'r') as fp:\n",
    "    dictionary_mp_eqpt = json.load(fp)\n",
    "for key in dictionary_mp_eqpt.keys():\n",
    "    dictionary_mp_eqpt[key].append(key)\n",
    "dictionary_mp_eqpt\n",
    "\n",
    "dictionary_mp_eqpt['tool_control'].append('programmable_logic_control')\n",
    "dictionary_mp_eqpt['motor_cooling_system'].append('duct')\n",
    "dictionary_mp_eqpt['motor_cooling_system'].append('air_duct')\n",
    "dictionary_mp_eqpt['motor_cooling_system'].append('arrestor')\n",
    "rem_ = ['fastener','seal']\n",
    "for v in dictionary_mp_eqpt.values():\n",
    "    for r_v in rem_:\n",
    "        if r_v in v:\n",
    "            v.remove(r_v)\n",
    "dictionary_mp_eqpt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ept_taxo_dict = {}\n",
    "for key, value in dictionary_mp_eqpt.items():\n",
    "    for val in value:\n",
    "        if val in [*vocab_dict.keys()]:\n",
    "            print(key, val)\n",
    "            print(vocab_dict[val])\n",
    "            ept_taxo_dict.setdefault(key,[]).append(val)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "eqpt_taxo_dict_combs = {}\n",
    "for key, value in ept_taxo_dict.items():\n",
    "    eqpt_taxo_dict_combs.setdefault(key,[]).append(list(combinations(value,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eqpt_taxo_dict_combs_synonyms = {}\n",
    "for key, value in eqpt_taxo_dict_combs.items():\n",
    "    for val1 in value:\n",
    "        for val2 in val1:\n",
    "            print(val2)\n",
    "            eqpt_taxo_dict_combs_synonyms.setdefault(key,[]).append(val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "all_val_flat_list = [item for sublist in [*ept_taxo_dict.values()] for item in sublist]\n",
    "eqpt_taxo_dict_combs_antonyms = {}\n",
    "for key, value in ept_taxo_dict.items():\n",
    "    rem_val = [item for item in all_val_flat_list if item not in value]\n",
    "    eqpt_taxo_dict_combs_antonyms.setdefault(key,[]).append(list(product(value,rem_val)))\n",
    "eqpt_taxo_dict_combs_antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eqpt_taxo_dict_combs_antonyms_ = {}\n",
    "for key, value in eqpt_taxo_dict_combs_antonyms.items():\n",
    "    for val1 in value:\n",
    "        for val2 in val1:\n",
    "            print(val2)\n",
    "            eqpt_taxo_dict_combs_antonyms_.setdefault(key,[]).append(val2)\n",
    "eqpt_taxo_dict_combs_antonyms_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_synonym_lst = [item for sublist in [*eqpt_taxo_dict_combs_synonyms.values()] for item in sublist]\n",
    "all_synonym_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this list would be the one from which we would extract negative pairs\n",
    "comparison_lst_ =[item for sublist in [*ept_taxo_dict.values()] for item in sublist]\n",
    "comparison_lst_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_google= np.load('Embeddings_generated_Google_25_MAY.npy')\n",
    "# norm_google=np.sqrt(np.sum(np.square(embeddings_google), axis=1))\n",
    "# embeddings_google_scaled=embeddings_google/norm_google[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_antonym_lst = [item for sublist in [*eqpt_taxo_dict_combs_antonyms.values()] for item in sublist]\n",
    "all_antonym_lst = sum(all_antonym_lst, [])\n",
    "all_antonym_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Idea is to create a subset of embeddings to look up from and store the trained vectors in other subset\n",
    "forward and reverse dictionary is to be created for the only for the tokens of interest \n",
    "'''\n",
    "flat_values = [item for sublist in [*ept_taxo_dict.values()] for item in sublist]\n",
    "\n",
    "fwd_subset = dict((k, vocab_dict[k]) for k in flat_values if k in vocab_dict)\n",
    "bwd_subset  = {v: k for k, v in fwd_subset.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "comparison_lst_\n",
    "comparison_indices_ = [vocab_dict[w] for w in comparison_lst_]\n",
    "comparison_indices_\n",
    "def extract_negative(embeddings, list_indices, syn_indicator_):\n",
    "    if syn_indicator_ ==True:\n",
    "        #embed_subset = tf.nn.embedding_lookup(embeddings, list_indices) #subsetting the embeddings for the words to cluster\n",
    "        #normalized = tf.nn.l2_normalize(embed_subset, dim = 1)\n",
    "        negative_examples_synonyms = []\n",
    "        for val in list_indices:\n",
    "            lst_comb= [val]\n",
    "            lst_1 = [w1 for w1 in comparison_indices_ if w1 not in lst_comb]\n",
    "            embedding_array_ = []\n",
    "            for w in lst_1:\n",
    "                #embedding_array_.append(tf.nn.embedding_lookup(embeddings, w))\n",
    "                embedding_array_.append(embeddings[w])\n",
    "            embedding_val = embeddings[val]\n",
    "            cos_lst = [cosine(embd_, embedding_val) for embd_ in embedding_array_]\n",
    "            indx_ = np.argmin(cos_lst)\n",
    "            neg_ = lst_1[indx_]\n",
    "            negative_examples_synonyms.append(neg_)\n",
    "        return negative_examples_synonyms\n",
    "    else:\n",
    "        negative_examples_antonyms = []\n",
    "        for val in list_indices:\n",
    "            lst_comb= [val]\n",
    "            lst_1 = [w1 for w1 in comparison_indices_ if w1 not in lst_comb]\n",
    "            embedding_array_ = []\n",
    "            for w in lst_1:\n",
    "                embedding_array_.append(embeddings[w])\n",
    "            embedding_val = embeddings[val]\n",
    "            cos_lst = [cosine(embd_, embedding_val) for embd_ in embedding_array_]\n",
    "            indx_ = np.argmax(cos_lst)\n",
    "            neg_ = lst_1[indx_]\n",
    "            negative_examples_antonyms.append(neg_)\n",
    "        return negative_examples_antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \"\"\"\n",
    "    Initialises the TensorFlow Attract-Repel model.\n",
    "    \"\"\"\n",
    "    attract_examples_lt = tf.placeholder(tf.int32, shape=(None)) # each element is the position of word vector.\n",
    "    attract_examples_rt = tf.placeholder(tf.int32, shape=(None)) # each element is the position of word vector.\n",
    "    \n",
    "    repel_examples_lt = tf.placeholder(tf.int32, shape=(None)) # each element is again the position of word vector.\n",
    "    repel_examples_rt = tf.placeholder(tf.int32, shape=(None)) # each element is again the position of word vector.\n",
    "    \n",
    "    negative_examples_attract_lt = tf.placeholder(tf.int32, shape=(None))\n",
    "    negative_examples_attract_rt = tf.placeholder(tf.int32, shape=(None))\n",
    "    \n",
    "    negative_examples_repel_lt = tf.placeholder(tf.int32, shape=(None))\n",
    "    negative_examples_repel_rt = tf.placeholder(tf.int32, shape=(None))\n",
    "    \n",
    "    attract_margin = tf.placeholder(\"float\")\n",
    "    repel_margin = tf.placeholder(\"float\")\n",
    "    regularisation_constant = tf.placeholder(\"float\")\n",
    "\n",
    "    # Initial (distributional) vectors. Needed for L2 regularisation.         \n",
    "    W_init = tf.constant(embeddings_google_scaled, name=\"W_init\")\n",
    "\n",
    "    # Variable storing the updated word vectors. \n",
    "    W_dynamic = tf.Variable(embeddings_google_scaled, name=\"W_dynamic\")\n",
    "    \n",
    "    # Attract Cost Function: \n",
    "\n",
    "    # placeholders for example pairs...\n",
    "    attract_examples_left = tf.nn.l2_normalize(tf.nn.embedding_lookup(W_dynamic, attract_examples_lt), 1) \n",
    "    attract_examples_right = tf.nn.l2_normalize(tf.nn.embedding_lookup(W_dynamic, attract_examples_rt), 1)\n",
    "\n",
    "    # and their respective negative examples:\n",
    "    negative_examples_attract_left = tf.nn.l2_normalize(tf.nn.embedding_lookup(W_dynamic, negative_examples_attract_lt), 1)\n",
    "    negative_examples_attract_right = tf.nn.l2_normalize(tf.nn.embedding_lookup(W_dynamic, negative_examples_attract_rt), 1)\n",
    "\n",
    "    # dot product between the example pairs. \n",
    "    attract_similarity_between_examples = tf.reduce_sum(tf.multiply(attract_examples_left, attract_examples_right), 1) \n",
    "\n",
    "    # dot product of each word in the example with its negative example. \n",
    "    attract_similarity_to_negatives_left = tf.reduce_sum(tf.multiply(attract_examples_left, negative_examples_attract_left), 1) \n",
    "    attract_similarity_to_negatives_right = tf.reduce_sum(tf.multiply(attract_examples_right, negative_examples_attract_right), 1)\n",
    "\n",
    "    # and the final Attract Cost Function (sans regularisation):\n",
    "    attract_cost = tf.nn.relu(attract_margin + attract_similarity_to_negatives_left - attract_similarity_between_examples) + \\\n",
    "                   tf.nn.relu(attract_margin + attract_similarity_to_negatives_right - attract_similarity_between_examples)\n",
    "\n",
    "    # Repel Cost Function: \n",
    "\n",
    "    # placeholders for example pairs...\n",
    "    repel_examples_left = tf.nn.l2_normalize(tf.nn.embedding_lookup(W_dynamic, repel_examples_lt), 1) # becomes batch_size X vector_dimension \n",
    "    repel_examples_right = tf.nn.l2_normalize(tf.nn.embedding_lookup(W_dynamic, repel_examples_rt), 1)\n",
    "\n",
    "    # and their respective negative examples:\n",
    "    negative_examples_repel_left  = tf.nn.l2_normalize(tf.nn.embedding_lookup(W_dynamic, negative_examples_repel_lt), 1)\n",
    "    negative_examples_repel_right = tf.nn.l2_normalize(tf.nn.embedding_lookup(W_dynamic, negative_examples_repel_rt), 1)\n",
    "\n",
    "    # dot product between the example pairs. \n",
    "    repel_similarity_between_examples = tf.reduce_sum(tf.multiply(repel_examples_left, repel_examples_right), 1) # becomes batch_size again, might need tf.squeeze\n",
    "\n",
    "    # dot product of each word in the example with its negative example. \n",
    "    repel_similarity_to_negatives_left = tf.reduce_sum(tf.multiply(repel_examples_left, negative_examples_repel_left), 1)\n",
    "    repel_similarity_to_negatives_right = tf.reduce_sum(tf.multiply(repel_examples_right, negative_examples_repel_right), 1)\n",
    "\n",
    "    # and the final Repel Cost Function (sans regularisation):\n",
    "    repel_cost = tf.nn.relu(repel_margin - repel_similarity_to_negatives_left + repel_similarity_between_examples) + \\\n",
    "                   tf.nn.relu(repel_margin - repel_similarity_to_negatives_right + repel_similarity_between_examples)\n",
    "\n",
    "    # The Regularisation Cost (separate for the two terms, depending on which one is called): \n",
    "\n",
    "    # load the original distributional vectors for the example pairs: \n",
    "    original_attract_examples_left = tf.nn.embedding_lookup(W_init, attract_examples_lt)\n",
    "    original_attract_examples_right = tf.nn.embedding_lookup(W_init, attract_examples_rt)\n",
    "\n",
    "    original_repel_examples_left = tf.nn.embedding_lookup(W_init, repel_examples_lt)\n",
    "    original_repel_examples_right = tf.nn.embedding_lookup(W_init, repel_examples_rt)\n",
    "\n",
    "    # and then define the respective regularisation costs:\n",
    "    regularisation_cost_attract = regularisation_constant * (tf.nn.l2_loss(original_attract_examples_left - attract_examples_left) + tf.nn.l2_loss(original_attract_examples_right - attract_examples_right))\n",
    "    attract_cost += regularisation_cost_attract\n",
    "\n",
    "    regularisation_cost_repel = regularisation_constant * (tf.nn.l2_loss(original_repel_examples_left - repel_examples_left) + tf.nn.l2_loss(original_repel_examples_right - repel_examples_right))\n",
    "    repel_cost += regularisation_cost_repel\n",
    "\n",
    "    # Finally, we define the training step functions for both steps. \n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    attract_grads = [tf.clip_by_value(grad, -2., 2.) for grad in tf.gradients(attract_cost, tvars)]\n",
    "    repel_grads = [tf.clip_by_value(grad, -2., 2.) for grad in tf.gradients(repel_cost, tvars)]\n",
    "\n",
    "    attract_optimiser = tf.train.AdagradOptimizer(0.05) \n",
    "    repel_optimiser = tf.train.AdagradOptimizer(0.05) \n",
    "\n",
    "    attract_cost_step = attract_optimiser.apply_gradients(zip(attract_grads, tvars))\n",
    "    repel_cost_step = repel_optimiser.apply_gradients(zip(repel_grads, tvars))\n",
    "    init = tf.global_variables_initializer()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "def run(graph, all_synonym_lst_ , all_anotnym_lst_, fwd_subset_,\n",
    "        num_steps,batch_size_,attract_margin_value,repel_margin_value,regularisation_constant_value):\n",
    "    loss_ = []\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      # We must initialize all variables before we use them.\n",
    "        \n",
    "        init.run()\n",
    "        print('Initialized')\n",
    "        current_iteration=0\n",
    "        average_loss = 0\n",
    "        \n",
    "        synonyms_left_ = [ seq[0] for seq in all_synonym_lst_ ]\n",
    "        synonyms_right_ = [ seq[1] for seq in all_synonym_lst_ ]\n",
    "        attract_indices_left_ = [fwd_subset_[w] for w in synonyms_left_]\n",
    "        attract_indices_right_ = [fwd_subset_[w] for w in synonyms_right_]\n",
    "        \n",
    "#         negative_attract_left_ = [ seq[0] for seq in negative_examples_synonyms_ ]\n",
    "#         negative_attract_right_ = [ seq[1] for seq in negative_examples_synonyms_ ]\n",
    "#         negative_attract_indices_left_ = [fwd_subset_[w] for w in negative_attract_left_]\n",
    "#         negative_attract_indices_right_ = [fwd_subset_[w] for w in negative_attract_right_]\n",
    "        \n",
    "        antonym_left_ = [ seq[0] for seq in all_anotnym_lst_ ]\n",
    "        antonym_right_ = [ seq[1] for seq in all_anotnym_lst_ ]\n",
    "        repel_indices_left_ = [fwd_subset_[w] for w in antonym_left_]\n",
    "        repel_indices_right_ = [fwd_subset_[w] for w in antonym_right_]\n",
    "        \n",
    "#         negative_repel_left_ = [ seq[0] for seq in negative_examples_antonyms_ ]\n",
    "#         negative_repel_right_ = [ seq[1] for seq in negative_examples_antonyms_ ]\n",
    "#         negative_repel_indices_left_ = [fwd_subset_[w] for w in negative_repel_left_]\n",
    "#         negative_repel_indices_right_ = [fwd_subset_[w] for w in negative_repel_right_]\n",
    "        \n",
    "        syn_cnt = len(synonyms_left_)\n",
    "        ant_cnt =  len(antonym_left_)\n",
    "        \n",
    "        syn_batch_ = int(syn_cnt/batch_size_)\n",
    "        ant_batch_ = int(ant_cnt/batch_size_)\n",
    "        \n",
    "        batch_per_epoch_ =  syn_batch_+ant_batch_\n",
    "        last_time = time.time()\n",
    "        average_loss = []\n",
    "        while current_iteration < int(num_steps):\n",
    "\n",
    "            # how many attract/repel batches we've done in this epoch so far.\n",
    "            antonym_counter = 0\n",
    "            synonym_counter = 0\n",
    "\n",
    "            order_of_synonyms = list(range(0, syn_cnt))\n",
    "            order_of_antonyms = list(range(0, ant_cnt))\n",
    "\n",
    "            random.shuffle(order_of_synonyms)\n",
    "            random.shuffle(order_of_antonyms)\n",
    "\n",
    "            # list of 0 where we run synonym batch, 1 where we run antonym batch\n",
    "            list_of_batch_types = [0] * batch_per_epoch_\n",
    "            list_of_batch_types[syn_batch_:] = [1] * ant_batch_ # all antonym batches to 1\n",
    "            random.shuffle(list_of_batch_types)\n",
    "        \n",
    "            if current_iteration == 0:\n",
    "                print(\"\\nStarting epoch:\", current_iteration+1, \"\\n\")\n",
    "            else:\n",
    "                print(\"\\nStarting epoch:\", current_iteration+1, \"Last epoch took:\", round(time.time() - last_time, 1), \"seconds. \\n\")\n",
    "                last_time = time.time()\n",
    "                \n",
    "            for batch_index in range(0, batch_per_epoch_):\n",
    "                syn_or_ant_batch = list_of_batch_types[batch_index]\n",
    "\n",
    "                if syn_or_ant_batch == 0:\n",
    "                    # do one synonymy batch:\n",
    "                    synonymy_examples_lt = [attract_indices_left_[order_of_synonyms[int(x)]] \n",
    "                                            for x in range(int(synonym_counter * batch_size_),int((synonym_counter+1) * batch_size_))]\n",
    "                    synonymy_examples_rt = [attract_indices_right_[order_of_synonyms[int(x)]] \n",
    "                                            for x in range(int(synonym_counter * batch_size_),int((synonym_counter+1) * batch_size_))]\n",
    "#                     current_negatives_syn_lt = [negative_attract_indices_left_[order_of_synonyms[int(x)]] \n",
    "#                                             for x in range(int(synonym_counter * batch_size_),int((synonym_counter+1) * batch_size_))]\n",
    "                    \n",
    "                    current_negatives_syn_lt = extract_negative(W_dynamic.eval(), synonymy_examples_lt, syn_indicator_ = True)\n",
    "                    current_negatives_syn_rt = extract_negative(W_dynamic.eval(), synonymy_examples_rt, syn_indicator_ = True)\n",
    "#                     current_negatives_syn_rt = [negative_attract_indices_right_[order_of_synonyms[int(x)]]\n",
    "#                                             for x in range(int(synonym_counter * batch_size_),int((synonym_counter+1) * batch_size_))]\n",
    "\n",
    "                    loss_val = session.run([attract_cost_step],\n",
    "                                  feed_dict={attract_examples_lt: synonymy_examples_lt,\n",
    "                                             attract_examples_rt: synonymy_examples_rt,\n",
    "                                             negative_examples_attract_lt: current_negatives_syn_lt,\n",
    "                                             negative_examples_attract_rt: current_negatives_syn_rt,\n",
    "                                             attract_margin: attract_margin_value, \n",
    "                                             regularisation_constant: regularisation_constant_value})\n",
    "                    synonym_counter += 1\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    antonym_examples_lt = [repel_indices_left_[order_of_antonyms[int(x)]] \n",
    "                                            for x in range(int(antonym_counter * batch_size_),int((antonym_counter+1) * batch_size_))]\n",
    "                    antonym_examples_rt = [repel_indices_right_[order_of_antonyms[int(x)]] \n",
    "                                            for x in range(int(antonym_counter * batch_size_),int((antonym_counter+1) * batch_size_))]\n",
    "#                     current_negatives_ant_lt = [negative_repel_indices_left_[order_of_antonyms[int(x)]] \n",
    "#                                             for x in range(int(antonym_counter * batch_size_),int((antonym_counter+1) * batch_size_))]\n",
    "#                     current_negatives_ant_rt = [negative_repel_indices_right_[order_of_antonyms[int(x)]]\n",
    "#                                             for x in range(int(antonym_counter * batch_size_),int((antonym_counter+1) * batch_size_))]\n",
    "                    \n",
    "                    current_negatives_ant_lt = extract_negative(W_dynamic.eval(), antonym_examples_lt, syn_indicator_ = False)\n",
    "                    current_negatives_ant_rt = extract_negative(W_dynamic.eval(), antonym_examples_rt, syn_indicator_ = False)\n",
    "                    \n",
    "                    loss_val = session.run([repel_cost_step],\n",
    "                                  feed_dict={repel_examples_lt: antonym_examples_lt,\n",
    "                                             repel_examples_rt: antonym_examples_rt,\n",
    "                                             negative_examples_repel_lt: current_negatives_ant_lt,\n",
    "                                             negative_examples_repel_rt: current_negatives_ant_rt,\n",
    "                                             repel_margin: repel_margin_value, \n",
    "                                             regularisation_constant: regularisation_constant_value})\n",
    "                    \n",
    "                    antonym_counter += 1\n",
    "                average_loss.append(loss_val)\n",
    "                loss_.append(average_loss)\n",
    "            current_iteration += 1\n",
    "#                     antonymy_examples = [self.antonyms[order_of_antonyms[x]] \n",
    "#                                          for x in range(antonym_counter * self.batch_size, (antonym_counter+1) * self.batch_size)]\n",
    "                    \n",
    "                    \n",
    "#                     current_negatives = self.extract_negative_examples(antonymy_examples, attract_batch=False)\n",
    "\n",
    "#                     self.sess.run([self.repel_cost_step], feed_dict={self.repel_examples: antonymy_examples, self.negative_examples_repel: current_negatives, \\\n",
    "#                                                                   self.repel_margin: self.repel_margin_value, self.regularisation_constant: self.regularisation_constant_value})\n",
    "        \n",
    "        final_embeddings = W_dynamic.eval()\n",
    "        initial_embeddings = W_init.eval()\n",
    "        return final_embeddings, initial_embeddings,loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Starting epoch: 1 \n",
      "\n",
      "\n",
      "Starting epoch: 2 Last epoch took: 92.3 seconds. \n",
      "\n",
      "\n",
      "Starting epoch: 3 Last epoch took: 36.3 seconds. \n",
      "\n",
      "\n",
      "Starting epoch: 4 Last epoch took: 31.3 seconds. \n",
      "\n",
      "\n",
      "Starting epoch: 5 Last epoch took: 34.2 seconds. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_embeddings,i_embeddings, loss = run(graph, all_synonym_lst, all_antonym_lst, fwd_subset,5.0,10.0,0.6,0.0, 0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(f_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(i_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(embeddings_google_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04303241, -0.02462361,  0.09771127,  0.0373629 ,  0.2545208 ,\n",
       "       -0.20046322,  0.01434526, -0.22431813,  0.17752431, -0.05215071,\n",
       "        0.16194609, -0.00568303, -0.01110241, -0.23066767, -0.06362169,\n",
       "        0.06779336,  0.1010457 ,  0.1491268 ,  0.08973411,  0.20283034,\n",
       "       -0.26765427, -0.11307473,  0.07173959,  0.24790318,  0.07961931,\n",
       "       -0.08651975, -0.03359356,  0.11608226, -0.15911503,  0.0277115 ,\n",
       "        0.21371303,  0.30736616, -0.1305928 , -0.1190127 ,  0.14981721,\n",
       "        0.05696556, -0.10413086, -0.16227709,  0.23613481, -0.13731743,\n",
       "        0.06654156,  0.01755162, -0.08489048,  0.03719721, -0.23736516,\n",
       "       -0.03598461,  0.02236842,  0.0324764 ,  0.06239328,  0.01444592,\n",
       "       -0.0507765 ,  0.13192989, -0.01740786, -0.08505104,  0.11261337,\n",
       "       -0.28377438, -0.19898446,  0.07600551,  0.49270594,  0.08079895,\n",
       "       -0.00872659, -0.05802275, -0.13353747,  0.13339959, -0.14529584,\n",
       "        0.053238  ,  0.23131913, -0.04504349,  0.25668713,  0.00697623,\n",
       "       -0.05074195,  0.15647033, -0.00912023,  0.03353409, -0.08416729,\n",
       "       -0.0381996 , -0.07996298,  0.12830904, -0.01670715,  0.00348962,\n",
       "        0.21433993, -0.15546884, -0.15843861,  0.08588517, -0.40781912,\n",
       "        0.17683893,  0.12675314, -0.10739285,  0.02782913,  0.12253726,\n",
       "        0.06928322,  0.03947053, -0.14980452, -0.11447578,  0.19008443,\n",
       "       -0.05109892,  0.05832748, -0.11655022,  0.34829798,  0.03565291],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_embeddings[vocab_dict['software']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.45223239e-02,  1.50411287e-02,  6.52041361e-02,  1.98040288e-02,\n",
       "        1.31744072e-01, -6.70875162e-02,  4.14898954e-02, -1.28355220e-01,\n",
       "        1.02836266e-01,  4.74901823e-03,  1.03906758e-01, -6.29567280e-02,\n",
       "        1.85546130e-02, -1.70985013e-01, -7.01478198e-02,  7.88412914e-02,\n",
       "        1.54341549e-01,  8.48739669e-02, -1.31275160e-02,  1.45644993e-01,\n",
       "       -1.84944555e-01, -1.14446588e-01,  1.08426251e-01,  1.17665768e-01,\n",
       "        2.18601674e-02, -4.96573783e-02,  2.16874089e-02,  3.40412110e-02,\n",
       "       -1.56485617e-01, -2.59354264e-02,  9.68853477e-03,  2.20606923e-01,\n",
       "       -1.67144224e-01, -5.85544631e-02,  1.19339362e-01,  4.71215248e-02,\n",
       "       -5.85405827e-02, -4.55728695e-02,  1.10530213e-01, -1.53758481e-01,\n",
       "       -1.31566689e-04,  3.28889713e-02, -5.92146516e-02, -5.45532443e-04,\n",
       "       -1.07480705e-01, -2.40181107e-02, -2.23337114e-02, -2.18833070e-02,\n",
       "        6.48802146e-02,  7.80777587e-03,  2.13588588e-02,  5.41414022e-02,\n",
       "       -2.10148841e-02,  1.08085368e-02,  2.32684612e-02, -2.33918607e-01,\n",
       "       -9.40918922e-02,  1.58598833e-02,  3.57163578e-01,  3.19387987e-02,\n",
       "        1.57210585e-02, -8.57547298e-03, -7.39037767e-02,  7.76011348e-02,\n",
       "       -7.66679272e-02,  1.23951407e-02,  1.34221315e-01, -6.63980246e-02,\n",
       "        1.57966390e-01,  3.37697342e-02, -3.42833847e-02,  1.15754619e-01,\n",
       "       -7.73527846e-02, -6.34009689e-02, -2.89078057e-02, -9.82473604e-03,\n",
       "       -5.23505695e-02,  8.56004804e-02, -7.17705190e-02, -1.48233296e-02,\n",
       "        1.56562731e-01, -7.35829398e-02, -1.54372409e-01,  6.80700839e-02,\n",
       "       -2.79992789e-01,  1.49118677e-01,  1.31856680e-01, -9.81471017e-02,\n",
       "       -4.71276976e-02,  1.54356975e-02,  2.63812076e-02,  4.59230095e-02,\n",
       "       -1.38773192e-02, -3.74254361e-02,  9.41103995e-02,  2.59909574e-02,\n",
       "        2.72187777e-03, -9.60153788e-02,  2.24200934e-01,  4.85884361e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_embeddings[vocab_dict['software']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.45223239e-02,  1.50411287e-02,  6.52041361e-02,  1.98040288e-02,\n",
       "        1.31744072e-01, -6.70875162e-02,  4.14898954e-02, -1.28355220e-01,\n",
       "        1.02836266e-01,  4.74901823e-03,  1.03906758e-01, -6.29567280e-02,\n",
       "        1.85546130e-02, -1.70985013e-01, -7.01478198e-02,  7.88412914e-02,\n",
       "        1.54341549e-01,  8.48739669e-02, -1.31275160e-02,  1.45644993e-01,\n",
       "       -1.84944555e-01, -1.14446588e-01,  1.08426251e-01,  1.17665768e-01,\n",
       "        2.18601674e-02, -4.96573783e-02,  2.16874089e-02,  3.40412110e-02,\n",
       "       -1.56485617e-01, -2.59354264e-02,  9.68853477e-03,  2.20606923e-01,\n",
       "       -1.67144224e-01, -5.85544631e-02,  1.19339362e-01,  4.71215248e-02,\n",
       "       -5.85405827e-02, -4.55728695e-02,  1.10530213e-01, -1.53758481e-01,\n",
       "       -1.31566689e-04,  3.28889713e-02, -5.92146516e-02, -5.45532443e-04,\n",
       "       -1.07480705e-01, -2.40181107e-02, -2.23337114e-02, -2.18833070e-02,\n",
       "        6.48802146e-02,  7.80777587e-03,  2.13588588e-02,  5.41414022e-02,\n",
       "       -2.10148841e-02,  1.08085368e-02,  2.32684612e-02, -2.33918607e-01,\n",
       "       -9.40918922e-02,  1.58598833e-02,  3.57163578e-01,  3.19387987e-02,\n",
       "        1.57210585e-02, -8.57547298e-03, -7.39037767e-02,  7.76011348e-02,\n",
       "       -7.66679272e-02,  1.23951407e-02,  1.34221315e-01, -6.63980246e-02,\n",
       "        1.57966390e-01,  3.37697342e-02, -3.42833847e-02,  1.15754619e-01,\n",
       "       -7.73527846e-02, -6.34009689e-02, -2.89078057e-02, -9.82473604e-03,\n",
       "       -5.23505695e-02,  8.56004804e-02, -7.17705190e-02, -1.48233296e-02,\n",
       "        1.56562731e-01, -7.35829398e-02, -1.54372409e-01,  6.80700839e-02,\n",
       "       -2.79992789e-01,  1.49118677e-01,  1.31856680e-01, -9.81471017e-02,\n",
       "       -4.71276976e-02,  1.54356975e-02,  2.63812076e-02,  4.59230095e-02,\n",
       "       -1.38773192e-02, -3.74254361e-02,  9.41103995e-02,  2.59909574e-02,\n",
       "        2.72187777e-03, -9.60153788e-02,  2.24200934e-01,  4.85884361e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_google_scaled[vocab_dict['software']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the generated word embeddings\n",
    "np.save('Embeddings_eqpt_taxo_competetive_with_Google_6_AUG_corrected', f_embeddings)\n",
    "\n",
    "#Saving the dictionary\n",
    "import json\n",
    "json = json.dumps(vocab_dict)\n",
    "f = open(\"word_index_dictionary_eqpt_taxo_competetive_with_Google_6_AUG_corrected.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_different_from(top_range, number_to_not_repeat):\n",
    "\n",
    "    result = random.randint(0, top_range-1)\n",
    "    while result == number_to_not_repeat:\n",
    "        result = random.randint(0, top_range-1)\n",
    "\n",
    "    return result\n",
    "def mix_sampling(list_of_examples, negative_examples):\n",
    "    \"\"\"\n",
    "    Converts half of the negative examples to random words from the batch (that are not in the given example pair).  \n",
    "    \"\"\"\n",
    "    mixed_negative_examples = []\n",
    "    batch_size = len(list_of_examples)\n",
    "\n",
    "    for idx, (left_idx, right_idx) in enumerate(negative_examples):\n",
    "\n",
    "        new_left = left_idx\n",
    "        new_right = right_idx\n",
    "\n",
    "        if random.random() >= 0.5:\n",
    "            new_left = list_of_examples[random_different_from(batch_size, idx)][random.randint(0, 1)]\n",
    "        \n",
    "        if random.random() >= 0.5:\n",
    "            new_right = list_of_examples[random_different_from(batch_size, idx)][random.randint(0, 1)]\n",
    "\n",
    "        mixed_negative_examples.append((new_left, new_right))\n",
    "\n",
    "    return mixed_negative_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \"\"\"\n",
    "    Initialises the TensorFlow Attract-Repel model.\n",
    "    \"\"\"\n",
    "    attract_examples_lt = tf.placeholder(tf.int32, shape=(None)) # each element is the position of word vector.\n",
    "    attract_examples_rt = tf.placeholder(tf.int32, shape=(None)) # each element is the position of word vector.\n",
    "    \n",
    "    repel_examples_lt = tf.placeholder(tf.int32, shape=(None)) # each element is again the position of word vector.\n",
    "    repel_examples_rt = tf.placeholder(tf.int32, shape=(None)) # each element is again the position of word vector.\n",
    "    \n",
    "    negative_examples_attract_lt = tf.placeholder(tf.int32, shape=(None))\n",
    "    negative_examples_attract_rt = tf.placeholder(tf.int32, shape=(None))\n",
    "    \n",
    "    negative_examples_repel_lt = tf.placeholder(tf.int32, shape=(None))\n",
    "    negative_examples_repel_rt = tf.placeholder(tf.int32, shape=(None))\n",
    "    \n",
    "    attract_margin = tf.placeholder(\"float\")\n",
    "    repel_margin = tf.placeholder(\"float\")\n",
    "    regularisation_constant = tf.placeholder(\"float\")\n",
    "\n",
    "    # Initial (distributional) vectors. Needed for L2 regularisation.         \n",
    "    W_init = tf.constant(embeddings_google_scaled, name=\"W_init\")\n",
    "\n",
    "    # Variable storing the updated word vectors. \n",
    "    W_dynamic = tf.Variable(embeddings_google_scaled, name=\"W_dynamic\")\n",
    "    \n",
    "    # Attract Cost Function: \n",
    "\n",
    "    # placeholders for example pairs...\n",
    "    attract_examples_left = tf.nn.l2_normalize(tf.nn.embedding_lookup(W_dynamic, attract_examples_lt), 1) \n",
    "    attract_examples_right = tf.nn.l2_normalize(tf.nn.embedding_lookup(W_dynamic, attract_examples_rt), 1)\n",
    "\n",
    "    # and their respective negative examples:\n",
    "    negative_examples_attract_left = tf.nn.l2_normalize(tf.nn.embedding_lookup(W_dynamic, negative_examples_attract_lt), 1)\n",
    "    negative_examples_attract_right = tf.nn.l2_normalize(tf.nn.embedding_lookup(W_dynamic, negative_examples_attract_rt), 1)\n",
    "\n",
    "    # dot product between the example pairs. \n",
    "    attract_similarity_between_examples = tf.reduce_sum(tf.multiply(attract_examples_left, attract_examples_right), 1) \n",
    "\n",
    "    # dot product of each word in the example with its negative example. \n",
    "    attract_similarity_to_negatives_left = tf.reduce_sum(tf.multiply(attract_examples_left, negative_examples_attract_left), 1) \n",
    "    attract_similarity_to_negatives_right = tf.reduce_sum(tf.multiply(attract_examples_right, negative_examples_attract_right), 1)\n",
    "\n",
    "    # and the final Attract Cost Function (sans regularisation):\n",
    "    attract_cost = tf.nn.relu(attract_margin + attract_similarity_to_negatives_left - attract_similarity_between_examples) + \\\n",
    "                   tf.nn.relu(attract_margin + attract_similarity_to_negatives_right - attract_similarity_between_examples)\n",
    "\n",
    "    # Repel Cost Function: \n",
    "\n",
    "    # placeholders for example pairs...\n",
    "    repel_examples_left = tf.nn.l2_normalize(tf.nn.embedding_lookup(W_dynamic, repel_examples_lt), 1) # becomes batch_size X vector_dimension \n",
    "    repel_examples_right = tf.nn.l2_normalize(tf.nn.embedding_lookup(W_dynamic, repel_examples_rt), 1)\n",
    "\n",
    "    # and their respective negative examples:\n",
    "    negative_examples_repel_left  = tf.nn.l2_normalize(tf.nn.embedding_lookup(W_dynamic, negative_examples_repel_lt), 1)\n",
    "    negative_examples_repel_right = tf.nn.l2_normalize(tf.nn.embedding_lookup(W_dynamic, negative_examples_repel_rt), 1)\n",
    "\n",
    "    # dot product between the example pairs. \n",
    "    repel_similarity_between_examples = tf.reduce_sum(tf.multiply(repel_examples_left, repel_examples_right), 1) # becomes batch_size again, might need tf.squeeze\n",
    "\n",
    "    # dot product of each word in the example with its negative example. \n",
    "    repel_similarity_to_negatives_left = tf.reduce_sum(tf.multiply(repel_examples_left, negative_examples_repel_left), 1)\n",
    "    repel_similarity_to_negatives_right = tf.reduce_sum(tf.multiply(repel_examples_right, negative_examples_repel_right), 1)\n",
    "\n",
    "    # and the final Repel Cost Function (sans regularisation):\n",
    "    repel_cost = tf.nn.relu(repel_margin - repel_similarity_to_negatives_left + repel_similarity_between_examples) + \\\n",
    "                   tf.nn.relu(repel_margin - repel_similarity_to_negatives_right + repel_similarity_between_examples)\n",
    "\n",
    "    # The Regularisation Cost (separate for the two terms, depending on which one is called): \n",
    "\n",
    "    # load the original distributional vectors for the example pairs: \n",
    "    original_attract_examples_left = tf.nn.embedding_lookup(W_init, attract_examples_lt)\n",
    "    original_attract_examples_right = tf.nn.embedding_lookup(W_init, attract_examples_rt)\n",
    "\n",
    "    original_repel_examples_left = tf.nn.embedding_lookup(W_init, repel_examples_lt)\n",
    "    original_repel_examples_right = tf.nn.embedding_lookup(W_init, repel_examples_rt)\n",
    "\n",
    "    # and then define the respective regularisation costs:\n",
    "    regularisation_cost_attract = regularisation_constant * (tf.nn.l2_loss(original_attract_examples_left - attract_examples_left) + tf.nn.l2_loss(original_attract_examples_right - attract_examples_right))\n",
    "    attract_cost += regularisation_cost_attract\n",
    "\n",
    "    regularisation_cost_repel = regularisation_constant * (tf.nn.l2_loss(original_repel_examples_left - repel_examples_left) + tf.nn.l2_loss(original_repel_examples_right - repel_examples_right))\n",
    "    repel_cost += regularisation_cost_repel\n",
    "\n",
    "    # Finally, we define the training step functions for both steps. \n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    attract_grads = [tf.clip_by_value(grad, -2., 2.) for grad in tf.gradients(attract_cost, tvars)]\n",
    "    repel_grads = [tf.clip_by_value(grad, -2., 2.) for grad in tf.gradients(repel_cost, tvars)]\n",
    "\n",
    "    attract_optimiser = tf.train.AdagradOptimizer(0.05) \n",
    "    repel_optimiser = tf.train.AdagradOptimizer(0.05) \n",
    "\n",
    "    attract_cost_step = attract_optimiser.apply_gradients(zip(attract_grads, tvars))\n",
    "    repel_cost_step = repel_optimiser.apply_gradients(zip(repel_grads, tvars))\n",
    "    init = tf.global_variables_initializer()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform\n",
    "def extract_negative_examples(list_left_batch,list_right_batch,graph, attract_batch = True):\n",
    "    \"\"\"\n",
    "    For each example in the minibatch, this method returns the closest vector which is not \n",
    "    in each words example pair. \n",
    "    \"\"\"\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        init.run()\n",
    "        list_of_representations = []\n",
    "        list_of_indices = []\n",
    "\n",
    "        representations = session.run([attract_examples_left, attract_examples_right], feed_dict={attract_examples_lt: list_left_batch,\n",
    "                                                                                                 attract_examples_rt:list_right_batch})\n",
    "        \n",
    "        \n",
    "        list_minibatch = []\n",
    "        for i in range(len(list_left_batch)):\n",
    "            list_minibatch.append((list_left_batch[i],list_right_batch[i]))\n",
    "        \n",
    "        for idx, (example_left, example_right) in enumerate(list_minibatch):\n",
    "\n",
    "            list_of_representations.append(representations[0][idx])\n",
    "            list_of_representations.append(representations[1][idx])\n",
    "\n",
    "            list_of_indices.append(example_left)\n",
    "            list_of_indices.append(example_right)\n",
    "\n",
    "        condensed_distance_list = pdist(list_of_representations, 'cosine') \n",
    "        square_distance_list = squareform(condensed_distance_list)   \n",
    "\n",
    "        if attract_batch: \n",
    "            default_value = 2.0 # value to set for given attract/repel pair, so that it can not be found as closest or furthest away. \n",
    "        else:\n",
    "            default_value = 0.0 # for antonyms, we want the opposite value from the synonym one. Cosine Distance is [0,2]. \n",
    "\n",
    "        for i in range(len(square_distance_list)):\n",
    "\n",
    "            square_distance_list[i,i]=default_value \n",
    "\n",
    "            if i % 2 == 0:\n",
    "                square_distance_list[i,i+1] = default_value \n",
    "            else:\n",
    "                square_distance_list[i,i-1] = default_value\n",
    "\n",
    "        if attract_batch:\n",
    "            negative_example_indices = np.argmin(square_distance_list,axis=1) # for each of the 100 elements, finds the index which has the minimal cosine distance (i.e. most similar). \n",
    "        else:\n",
    "            negative_example_indices = np.argmax(square_distance_list, axis=1) # for antonyms, find the least similar one. \n",
    "\n",
    "        negative_examples = []\n",
    "\n",
    "        for idx in range(len(list_minibatch)):\n",
    "\n",
    "            negative_example_left = list_of_indices[negative_example_indices[2 * idx]] \n",
    "            negative_example_right = list_of_indices[negative_example_indices[2 * idx + 1]]\n",
    "\n",
    "            negative_examples.append((negative_example_left, negative_example_right))            \n",
    "\n",
    "        negative_examples = mix_sampling(list_minibatch, negative_examples)\n",
    "        \n",
    "        neg_left_=[]\n",
    "        neg_right_=[]\n",
    "        for a,b in negative_examples:\n",
    "            neg_left_.append(a)\n",
    "            neg_right_.append(b)\n",
    "        \n",
    "        return neg_left_,neg_right_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "def run(graph, all_synonym_lst_ , all_anotnym_lst_, fwd_subset_,\n",
    "        num_steps,batch_size_,attract_margin_value,repel_margin_value,regularisation_constant_value):\n",
    "    loss_ = []\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      # We must initialize all variables before we use them.\n",
    "        \n",
    "        init.run()\n",
    "        print('Initialized')\n",
    "        current_iteration=0\n",
    "        average_loss = 0\n",
    "        \n",
    "        synonyms_left_ = [ seq[0] for seq in all_synonym_lst_ ]\n",
    "        synonyms_right_ = [ seq[1] for seq in all_synonym_lst_ ]\n",
    "        attract_indices_left_ = [fwd_subset_[w] for w in synonyms_left_]\n",
    "        attract_indices_right_ = [fwd_subset_[w] for w in synonyms_right_]\n",
    "        \n",
    "#         negative_attract_left_ = [ seq[0] for seq in negative_examples_synonyms_ ]\n",
    "#         negative_attract_right_ = [ seq[1] for seq in negative_examples_synonyms_ ]\n",
    "#         negative_attract_indices_left_ = [fwd_subset_[w] for w in negative_attract_left_]\n",
    "#         negative_attract_indices_right_ = [fwd_subset_[w] for w in negative_attract_right_]\n",
    "        \n",
    "        antonym_left_ = [ seq[0] for seq in all_anotnym_lst_ ]\n",
    "        antonym_right_ = [ seq[1] for seq in all_anotnym_lst_ ]\n",
    "        repel_indices_left_ = [fwd_subset_[w] for w in antonym_left_]\n",
    "        repel_indices_right_ = [fwd_subset_[w] for w in antonym_right_]\n",
    "        \n",
    "#         negative_repel_left_ = [ seq[0] for seq in negative_examples_antonyms_ ]\n",
    "#         negative_repel_right_ = [ seq[1] for seq in negative_examples_antonyms_ ]\n",
    "#         negative_repel_indices_left_ = [fwd_subset_[w] for w in negative_repel_left_]\n",
    "#         negative_repel_indices_right_ = [fwd_subset_[w] for w in negative_repel_right_]\n",
    "        \n",
    "        syn_cnt = len(synonyms_left_)\n",
    "        ant_cnt =  len(antonym_left_)\n",
    "        \n",
    "        syn_batch_ = int(syn_cnt/batch_size_)\n",
    "        ant_batch_ = int(ant_cnt/batch_size_)\n",
    "        \n",
    "        batch_per_epoch_ =  syn_batch_+ant_batch_\n",
    "        last_time = time.time()\n",
    "        average_loss = []\n",
    "        while current_iteration < int(num_steps):\n",
    "\n",
    "            # how many attract/repel batches we've done in this epoch so far.\n",
    "            antonym_counter = 0\n",
    "            synonym_counter = 0\n",
    "\n",
    "            order_of_synonyms = list(range(0, syn_cnt))\n",
    "            order_of_antonyms = list(range(0, ant_cnt))\n",
    "\n",
    "            random.shuffle(order_of_synonyms)\n",
    "            random.shuffle(order_of_antonyms)\n",
    "\n",
    "            # list of 0 where we run synonym batch, 1 where we run antonym batch\n",
    "            list_of_batch_types = [0] * batch_per_epoch_\n",
    "            list_of_batch_types[syn_batch_:] = [1] * ant_batch_ # all antonym batches to 1\n",
    "            random.shuffle(list_of_batch_types)\n",
    "        \n",
    "            if current_iteration == 0:\n",
    "                print(\"\\nStarting epoch:\", current_iteration+1, \"\\n\")\n",
    "            else:\n",
    "                print(\"\\nStarting epoch:\", current_iteration+1, \"Last epoch took:\", round(time.time() - last_time, 1), \"seconds. \\n\")\n",
    "                last_time = time.time()\n",
    "                \n",
    "            for batch_index in range(0, batch_per_epoch_):\n",
    "                syn_or_ant_batch = list_of_batch_types[batch_index]\n",
    "\n",
    "                if syn_or_ant_batch == 0:\n",
    "                    # do one synonymy batch:\n",
    "                    synonymy_examples_lt = [attract_indices_left_[order_of_synonyms[int(x)]] \n",
    "                                            for x in range(int(synonym_counter * batch_size_),int((synonym_counter+1) * batch_size_))]\n",
    "                    synonymy_examples_rt = [attract_indices_right_[order_of_synonyms[int(x)]] \n",
    "                                            for x in range(int(synonym_counter * batch_size_),int((synonym_counter+1) * batch_size_))]\n",
    "#                     current_negatives_syn_lt = [negative_attract_indices_left_[order_of_synonyms[int(x)]] \n",
    "#                                             for x in range(int(synonym_counter * batch_size_),int((synonym_counter+1) * batch_size_))]\n",
    "                    \n",
    "                    current_negatives_syn_lt, current_negatives_syn_rt = extract_negative_examples(synonymy_examples_lt,synonymy_examples_rt,\n",
    "                                                                                graph=graph,attract_batch = True)\n",
    "                    loss_val = session.run([attract_cost_step],\n",
    "                                  feed_dict={attract_examples_lt: synonymy_examples_lt,\n",
    "                                             attract_examples_rt: synonymy_examples_rt,\n",
    "                                             negative_examples_attract_lt: current_negatives_syn_lt,\n",
    "                                             negative_examples_attract_rt: current_negatives_syn_rt,\n",
    "                                             attract_margin: attract_margin_value, \n",
    "                                             regularisation_constant: regularisation_constant_value})\n",
    "                    synonym_counter += 1\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    antonym_examples_lt = [repel_indices_left_[order_of_antonyms[int(x)]] \n",
    "                                            for x in range(int(antonym_counter * batch_size_),int((antonym_counter+1) * batch_size_))]\n",
    "                    antonym_examples_rt = [repel_indices_right_[order_of_antonyms[int(x)]] \n",
    "                                            for x in range(int(antonym_counter * batch_size_),int((antonym_counter+1) * batch_size_))]\n",
    "#                     current_negatives_ant_lt = [negative_repel_indices_left_[order_of_antonyms[int(x)]] \n",
    "#                                             for x in range(int(antonym_counter * batch_size_),int((antonym_counter+1) * batch_size_))]\n",
    "#                     current_negatives_ant_rt = [negative_repel_indices_right_[order_of_antonyms[int(x)]]\n",
    "#                                             for x in range(int(antonym_counter * batch_size_),int((antonym_counter+1) * batch_size_))]\n",
    "                    \n",
    "                    current_negatives_ant_lt,current_negatives_ant_rt = extract_negative_examples(antonym_examples_lt,antonym_examples_rt,\n",
    "                                                                                graph=graph,attract_batch = False)\n",
    "                    \n",
    "                    loss_val = session.run([repel_cost_step],\n",
    "                                  feed_dict={repel_examples_lt: antonym_examples_lt,\n",
    "                                             repel_examples_rt: antonym_examples_rt,\n",
    "                                             negative_examples_repel_lt: current_negatives_ant_lt,\n",
    "                                             negative_examples_repel_rt: current_negatives_ant_rt,\n",
    "                                             repel_margin: repel_margin_value, \n",
    "                                             regularisation_constant: regularisation_constant_value})\n",
    "                    \n",
    "                    antonym_counter += 1\n",
    "                average_loss.append(loss_val)\n",
    "                loss_.append(average_loss)\n",
    "            current_iteration += 1\n",
    "#                     antonymy_examples = [self.antonyms[order_of_antonyms[x]] \n",
    "#                                          for x in range(antonym_counter * self.batch_size, (antonym_counter+1) * self.batch_size)]\n",
    "                    \n",
    "                    \n",
    "#                     current_negatives = self.extract_negative_examples(antonymy_examples, attract_batch=False)\n",
    "\n",
    "#                     self.sess.run([self.repel_cost_step], feed_dict={self.repel_examples: antonymy_examples, self.negative_examples_repel: current_negatives, \\\n",
    "#                                                                   self.repel_margin: self.repel_margin_value, self.regularisation_constant: self.regularisation_constant_value})\n",
    "        \n",
    "        final_embeddings = W_dynamic.eval()\n",
    "        initial_embeddings = W_init.eval()\n",
    "        return final_embeddings, initial_embeddings,loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Starting epoch: 1 \n",
      "\n",
      "\n",
      "Starting epoch: 2 Last epoch took: 860.3 seconds. \n",
      "\n",
      "\n",
      "Starting epoch: 3 Last epoch took: 731.7 seconds. \n",
      "\n",
      "\n",
      "Starting epoch: 4 Last epoch took: 807.8 seconds. \n",
      "\n",
      "\n",
      "Starting epoch: 5 Last epoch took: 679.2 seconds. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_embeddings,i_embeddings, loss = run(graph, all_synonym_lst, all_antonym_lst, fwd_subset,5.0,10.0,0.6,0.0, 0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43642961978912354\n",
      "0.43642961978912354\n",
      "0.3400229811668396\n",
      "0.3400229811668396\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "print(cosine(f_embeddings[dictionary['piston']],f_embeddings[dictionary['liner']]))\n",
    "print(cosine(i_embeddings[dictionary['piston']],i_embeddings[dictionary['liner']]))\n",
    "print(cosine(f_embeddings[dictionary['module']],f_embeddings[dictionary['spring']]))\n",
    "print(cosine(i_embeddings[dictionary['module']],i_embeddings[dictionary['spring']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the generated word embeddings\n",
    "np.save('Embeddings_eqpt_taxo_competetive_with_Google_10_AUG_updated_neg_sam', f_embeddings)\n",
    "\n",
    "#Saving the dictionary\n",
    "import json\n",
    "json = json.dumps(vocab_dict)\n",
    "f = open(\"word_index_dictionary_eqpt_taxo_competetive_with_Google_10_AUG_updated_neg_sam.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
